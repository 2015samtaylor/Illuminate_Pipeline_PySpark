{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/12 23:13:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/01/12 23:13:04 INFO SharedState: Warehouse path is 'file:/home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/spark-warehouse'.\n",
      "25/01/12 23:13:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on icef-instance-2.us-west2-a.c.icef-437920.internal:45131 in memory (size: 3.9 KiB, free: 366.3 MiB)\n",
      "25/01/12 23:13:05 INFO BlockManagerInfo: Removed broadcast_2_piece0 on icef-instance-2.us-west2-a.c.icef-437920.internal:45131 in memory (size: 3.9 KiB, free: 366.3 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-Jan-25 23:13:06 - \n",
      "\n",
      "-------------New Illuminate Operations Logging Instance\n",
      "12-Jan-25 23:13:06 - Calling API token endpoint\n",
      "12-Jan-25 23:13:06 - Succesfully retrieved API token\n",
      "12-Jan-25 23:13:06 - Fetching data from https://icefps.illuminateed.com/live/rest_server.php/Api/Assessments/?page=1&limit=1000\n",
      "12-Jan-25 23:13:07 - Here is the total num of pages on this endpoint 2\n",
      "12-Jan-25 23:13:07 - Fetching data from https://icefps.illuminateed.com/live/rest_server.php/Api/Assessments/?page=2&limit=1000\n",
      "12-Jan-25 23:13:07 - Here is the total num of pages on this endpoint 2\n",
      "12-Jan-25 23:13:07 - Looped through 2 pages. Results for func get_all_assessments_metadata output into DataFrame\n",
      "12-Jan-25 23:13:07 - Here is the length of the assessment_id_list variable 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/12 23:13:07 INFO SparkContext: Starting job: collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226\n",
      "25/01/12 23:13:07 INFO DAGScheduler: Got job 3 (collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226) with 4 output partitions\n",
      "25/01/12 23:13:07 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226)\n",
      "25/01/12 23:13:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/01/12 23:13:07 INFO DAGScheduler: Missing parents: List()\n",
      "25/01/12 23:13:07 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226), which has no missing parents\n",
      "25/01/12 23:13:07 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.2 KiB, free 366.3 MiB)\n",
      "25/01/12 23:13:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 366.3 MiB)\n",
      "25/01/12 23:13:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on icef-instance-2.us-west2-a.c.icef-437920.internal:45131 (size: 3.9 KiB, free: 366.3 MiB)\n",
      "25/01/12 23:13:07 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "25/01/12 23:13:07 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 3 (PythonRDD[7] at collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "25/01/12 23:13:07 INFO TaskSchedulerImpl: Adding task set 3.0 with 4 tasks resource profile 0\n",
      "25/01/12 23:13:07 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 12) (icef-instance-2.us-west2-a.c.icef-437920.internal, executor driver, partition 0, PROCESS_LOCAL, 9209 bytes) \n",
      "25/01/12 23:13:07 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 13) (icef-instance-2.us-west2-a.c.icef-437920.internal, executor driver, partition 1, PROCESS_LOCAL, 9212 bytes) \n",
      "25/01/12 23:13:07 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 14) (icef-instance-2.us-west2-a.c.icef-437920.internal, executor driver, partition 2, PROCESS_LOCAL, 9209 bytes) \n",
      "25/01/12 23:13:07 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 15) (icef-instance-2.us-west2-a.c.icef-437920.internal, executor driver, partition 3, PROCESS_LOCAL, 9251 bytes) \n",
      "25/01/12 23:13:07 INFO Executor: Running task 1.0 in stage 3.0 (TID 13)\n",
      "25/01/12 23:13:07 INFO Executor: Running task 0.0 in stage 3.0 (TID 12)\n",
      "25/01/12 23:13:07 INFO Executor: Running task 2.0 in stage 3.0 (TID 14)\n",
      "25/01/12 23:13:07 INFO Executor: Running task 3.0 in stage 3.0 (TID 15)\n",
      "25/01/12 23:13:11 INFO PythonRunner: Times: total = 3585, boot = -22120, init = 22128, finish = 3577\n",
      "25/01/12 23:13:11 INFO Executor: Finished task 0.0 in stage 3.0 (TID 12). 15402 bytes result sent to driver\n",
      "25/01/12 23:13:11 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 12) in 3630 ms on icef-instance-2.us-west2-a.c.icef-437920.internal (executor driver) (1/4)\n",
      "25/01/12 23:13:11 INFO PythonRunner: Times: total = 3606, boot = -22153, init = 22156, finish = 3603\n",
      "25/01/12 23:13:11 INFO Executor: Finished task 1.0 in stage 3.0 (TID 13). 15405 bytes result sent to driver\n",
      "25/01/12 23:13:11 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 13) in 3649 ms on icef-instance-2.us-west2-a.c.icef-437920.internal (executor driver) (2/4)\n",
      "25/01/12 23:13:11 INFO PythonRunner: Times: total = 3706, boot = -22155, init = 22167, finish = 3694\n",
      "25/01/12 23:13:11 INFO Executor: Finished task 2.0 in stage 3.0 (TID 14). 49128 bytes result sent to driver\n",
      "25/01/12 23:13:11 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 14) in 3746 ms on icef-instance-2.us-west2-a.c.icef-437920.internal (executor driver) (3/4)\n",
      "25/01/12 23:13:11 INFO PythonRunner: Times: total = 3943, boot = -22084, init = 22102, finish = 3925\n",
      "25/01/12 23:13:11 INFO Executor: Finished task 3.0 in stage 3.0 (TID 15). 16287 bytes result sent to driver\n",
      "25/01/12 23:13:11 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 15) in 3994 ms on icef-instance-2.us-west2-a.c.icef-437920.internal (executor driver) (4/4)\n",
      "25/01/12 23:13:11 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "25/01/12 23:13:11 INFO DAGScheduler: ResultStage 3 (collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226) finished in 4.018 s\n",
      "25/01/12 23:13:11 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/01/12 23:13:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "25/01/12 23:13:11 INFO DAGScheduler: Job 3 finished: collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226, took 4.040166 s\n",
      "25/01/12 23:13:11 INFO SparkContext: Starting job: collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226\n",
      "25/01/12 23:13:11 INFO DAGScheduler: Got job 4 (collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226) with 4 output partitions\n",
      "25/01/12 23:13:11 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226)\n",
      "25/01/12 23:13:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/01/12 23:13:11 INFO DAGScheduler: Missing parents: List()\n",
      "25/01/12 23:13:11 INFO DAGScheduler: Submitting ResultStage 4 (PythonRDD[9] at collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226), which has no missing parents\n",
      "25/01/12 23:13:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.2 KiB, free 366.3 MiB)\n",
      "25/01/12 23:13:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 366.3 MiB)\n",
      "25/01/12 23:13:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on icef-instance-2.us-west2-a.c.icef-437920.internal:45131 (size: 3.9 KiB, free: 366.3 MiB)\n",
      "25/01/12 23:13:11 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "25/01/12 23:13:11 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 4 (PythonRDD[9] at collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "25/01/12 23:13:11 INFO TaskSchedulerImpl: Adding task set 4.0 with 4 tasks resource profile 0\n",
      "25/01/12 23:13:11 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 16) (icef-instance-2.us-west2-a.c.icef-437920.internal, executor driver, partition 0, PROCESS_LOCAL, 9209 bytes) \n",
      "25/01/12 23:13:11 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 17) (icef-instance-2.us-west2-a.c.icef-437920.internal, executor driver, partition 1, PROCESS_LOCAL, 9212 bytes) \n",
      "25/01/12 23:13:11 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 18) (icef-instance-2.us-west2-a.c.icef-437920.internal, executor driver, partition 2, PROCESS_LOCAL, 9209 bytes) \n",
      "25/01/12 23:13:11 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 19) (icef-instance-2.us-west2-a.c.icef-437920.internal, executor driver, partition 3, PROCESS_LOCAL, 9251 bytes) \n",
      "25/01/12 23:13:11 INFO Executor: Running task 0.0 in stage 4.0 (TID 16)\n",
      "25/01/12 23:13:11 INFO Executor: Running task 1.0 in stage 4.0 (TID 17)\n",
      "25/01/12 23:13:11 INFO Executor: Running task 2.0 in stage 4.0 (TID 18)\n",
      "25/01/12 23:13:11 INFO Executor: Running task 3.0 in stage 4.0 (TID 19)\n",
      "25/01/12 23:13:15 INFO PythonRunner: Times: total = 3494, boot = -410, init = 421, finish = 3483\n",
      "25/01/12 23:13:15 INFO Executor: Finished task 0.0 in stage 4.0 (TID 16). 15417 bytes result sent to driver\n",
      "25/01/12 23:13:15 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 16) in 3513 ms on icef-instance-2.us-west2-a.c.icef-437920.internal (executor driver) (1/4)\n",
      "25/01/12 23:13:15 INFO PythonRunner: Times: total = 3712, boot = -321, init = 339, finish = 3694\n",
      "25/01/12 23:13:15 INFO Executor: Finished task 2.0 in stage 4.0 (TID 18). 216325 bytes result sent to driver\n",
      "25/01/12 23:13:15 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 18) in 3738 ms on icef-instance-2.us-west2-a.c.icef-437920.internal (executor driver) (2/4)\n",
      "25/01/12 23:13:15 INFO PythonRunner: Times: total = 3800, boot = -424, init = 457, finish = 3767\n",
      "25/01/12 23:13:15 INFO Executor: Finished task 1.0 in stage 4.0 (TID 17). 116072 bytes result sent to driver\n",
      "25/01/12 23:13:15 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 17) in 3820 ms on icef-instance-2.us-west2-a.c.icef-437920.internal (executor driver) (3/4)\n",
      "25/01/12 23:13:15 INFO PythonRunner: Times: total = 3985, boot = -80, init = 108, finish = 3957\n",
      "25/01/12 23:13:15 INFO Executor: Finished task 3.0 in stage 4.0 (TID 19). 309747 bytes result sent to driver\n",
      "25/01/12 23:13:15 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 19) in 4026 ms on icef-instance-2.us-west2-a.c.icef-437920.internal (executor driver) (4/4)\n",
      "25/01/12 23:13:15 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "25/01/12 23:13:15 INFO DAGScheduler: ResultStage 4 (collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226) finished in 4.045 s\n",
      "25/01/12 23:13:15 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/01/12 23:13:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "25/01/12 23:13:15 INFO DAGScheduler: Job 4 finished: collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226, took 4.056103 s\n",
      "25/01/12 23:13:15 INFO SparkContext: Starting job: collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226\n",
      "25/01/12 23:13:15 INFO DAGScheduler: Got job 5 (collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226) with 4 output partitions\n",
      "25/01/12 23:13:15 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226)\n",
      "25/01/12 23:13:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/01/12 23:13:15 INFO DAGScheduler: Missing parents: List()\n",
      "25/01/12 23:13:15 INFO DAGScheduler: Submitting ResultStage 5 (PythonRDD[11] at collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226), which has no missing parents\n",
      "25/01/12 23:13:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 6.2 KiB, free 366.3 MiB)\n",
      "25/01/12 23:13:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 366.3 MiB)\n",
      "25/01/12 23:13:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on icef-instance-2.us-west2-a.c.icef-437920.internal:45131 (size: 3.9 KiB, free: 366.3 MiB)\n",
      "25/01/12 23:13:15 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "25/01/12 23:13:15 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 5 (PythonRDD[11] at collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "25/01/12 23:13:15 INFO TaskSchedulerImpl: Adding task set 5.0 with 4 tasks resource profile 0\n",
      "25/01/12 23:13:15 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 20) (icef-instance-2.us-west2-a.c.icef-437920.internal, executor driver, partition 0, PROCESS_LOCAL, 9209 bytes) \n",
      "25/01/12 23:13:15 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 21) (icef-instance-2.us-west2-a.c.icef-437920.internal, executor driver, partition 1, PROCESS_LOCAL, 9212 bytes) \n",
      "25/01/12 23:13:15 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 22) (icef-instance-2.us-west2-a.c.icef-437920.internal, executor driver, partition 2, PROCESS_LOCAL, 9209 bytes) \n",
      "25/01/12 23:13:15 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 23) (icef-instance-2.us-west2-a.c.icef-437920.internal, executor driver, partition 3, PROCESS_LOCAL, 9251 bytes) \n",
      "25/01/12 23:13:15 INFO Executor: Running task 1.0 in stage 5.0 (TID 21)\n",
      "25/01/12 23:13:15 INFO Executor: Running task 0.0 in stage 5.0 (TID 20)\n",
      "25/01/12 23:13:15 INFO Executor: Running task 3.0 in stage 5.0 (TID 23)\n",
      "25/01/12 23:13:15 INFO Executor: Running task 2.0 in stage 5.0 (TID 22)\n",
      "25/01/12 23:13:19 INFO PythonRunner: Times: total = 3651, boot = -91, init = 101, finish = 3641\n",
      "25/01/12 23:13:19 INFO Executor: Finished task 0.0 in stage 5.0 (TID 20). 15432 bytes result sent to driver\n",
      "25/01/12 23:13:19 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 20) in 3668 ms on icef-instance-2.us-west2-a.c.icef-437920.internal (executor driver) (1/4)\n",
      "25/01/12 23:13:19 INFO PythonRunner: Times: total = 3670, boot = -405, init = 420, finish = 3655\n",
      "25/01/12 23:13:19 INFO Executor: Finished task 2.0 in stage 5.0 (TID 22). 45864 bytes result sent to driver\n",
      "25/01/12 23:13:19 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 22) in 3682 ms on icef-instance-2.us-west2-a.c.icef-437920.internal (executor driver) (2/4)\n",
      "25/01/12 23:13:19 INFO PythonRunner: Times: total = 3752, boot = -341, init = 361, finish = 3732\n",
      "25/01/12 23:13:19 INFO Executor: Finished task 1.0 in stage 5.0 (TID 21). 22907 bytes result sent to driver\n",
      "25/01/12 23:13:19 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 21) in 3782 ms on icef-instance-2.us-west2-a.c.icef-437920.internal (executor driver) (3/4)\n",
      "25/01/12 23:13:19 INFO PythonRunner: Times: total = 4061, boot = -644, init = 652, finish = 4053\n",
      "25/01/12 23:13:19 INFO Executor: Finished task 3.0 in stage 5.0 (TID 23). 29663 bytes result sent to driver\n",
      "25/01/12 23:13:19 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 23) in 4074 ms on icef-instance-2.us-west2-a.c.icef-437920.internal (executor driver) (4/4)\n",
      "25/01/12 23:13:19 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "25/01/12 23:13:19 INFO DAGScheduler: ResultStage 5 (collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226) finished in 4.103 s\n",
      "25/01/12 23:13:19 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/01/12 23:13:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "25/01/12 23:13:19 INFO DAGScheduler: Job 5 finished: collect at /home/g2015samtaylor/airflow/git_directory/pyspark_local/ICEF_Illuminate/modules/assessments_endpoints.py:226, took 4.116866 s\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-Jan-25 23:13:20 - Assessment results fetched and processed.\n",
      "12-Jan-25 23:13:20 - Sending data for 24-25 school year\n",
      "12-Jan-25 23:13:20 - assessment_results_group.csv saved to /home/g2015samtaylor/illuminate\n",
      "12-Jan-25 23:13:20 - assessment_results_combined.csv saved to /home/g2015samtaylor/illuminate\n",
      "12-Jan-25 23:13:20 - illuminate_assessment_results.csv saved to /home/g2015samtaylor/views\n",
      "12-Jan-25 23:13:20 - assessments_metadata,csv saved to /home/g2015samtaylor/illuminate\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from modules.auth import *\n",
    "from modules.assessments_endpoints import *\n",
    "from modules.frame_transformations import *\n",
    "from modules.config import base_url_illuminate\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from pyspark import RDD\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"API Request Parallelization\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "\n",
    "# Configure logging to use StreamHandler for stdout\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Adjust as needed (e.g., DEBUG, WARNING)\n",
    "    format=\"%(asctime)s - %(message)s\",  # Log format\n",
    "    datefmt=\"%d-%b-%y %H:%M:%S\",  # Date format\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)  # Direct logs to stdout\n",
    "    ],\n",
    "    force=True  # Ensures existing handlers are replaced\n",
    ")\n",
    "\n",
    "\n",
    "def get_assessment_results(spark, save_path, view_path, years_data, start_date, end_date_override=None):\n",
    "    logging.info('\\n\\n-------------New Illuminate Operations Logging Instance')\n",
    "\n",
    "    try:\n",
    "        access_token, expires_in = get_access_token()\n",
    "\n",
    "        assessments_df, assessment_id_list = get_all_assessments_metadata(access_token)\n",
    "        assessment_id_list = assessment_id_list[:100] #for testing\n",
    "        missing_ids_from_metadata = ['114845', '141498'] # Add assessments that are not present in assessements metadata\n",
    "        assessment_id_list = list(set(assessment_id_list + missing_ids_from_metadata))\n",
    "        logging.info(f'Here is the length of the assessment_id_list variable {len(assessment_id_list)}')\n",
    "\n",
    "        test_results_group, log_results_group = parallel_get_assessment_scores(spark, access_token, assessment_id_list, 'Group', start_date, end_date_override=None)\n",
    "        test_results_standard, log_results_standard = parallel_get_assessment_scores(spark, access_token, assessment_id_list, 'Standard', start_date, end_date_override)\n",
    "        test_results_no_standard, log_results_no_standard = parallel_get_assessment_scores(spark, access_token, assessment_id_list, 'No_Standard', start_date, end_date_override)\n",
    " \n",
    "        test_results_combined = bring_together_test_results(test_results_no_standard, test_results_standard)\n",
    "        test_results_view = create_test_results_view(test_results_combined, years_data) #add in grade level col, string matching\n",
    "        logging.info(\"Assessment results fetched and processed.\")\n",
    "\n",
    "        \n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        if years_data == '23-24':\n",
    "            logging.info(f'Sending data for {years_data} school year')\n",
    "            send_to_local(save_path, test_results_group, 'assessment_results_group_historical.csv')\n",
    "            send_to_local(save_path, test_results_combined, 'assessment_results_combined_historical.csv')\n",
    "            send_to_local(view_path, test_results_view, 'illuminate_assessment_results_historical.csv')\n",
    "            \n",
    "        elif years_data == '24-25':\n",
    "            logging.info(f'Sending data for {years_data} school year')\n",
    "            send_to_local(save_path, test_results_group, 'assessment_results_group.csv')\n",
    "            send_to_local(save_path, test_results_combined, 'assessment_results_combined.csv')\n",
    "            send_to_local(view_path, test_results_view, 'illuminate_assessment_results.csv')\n",
    "        else:\n",
    "            raise ValueError(f'Unexpected value for years variable data {years_data}')\n",
    "        \n",
    "        #No matter what update assessments_metadata file to display available assessments\n",
    "        send_to_local(save_path, assessments_df, 'assessments_metadata,csv')\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching assessment results: {e}\")\n",
    "        raise AirflowException(\"Failed to fetch and process assessment results\")\n",
    "\n",
    "\n",
    "get_assessment_results(spark,\n",
    "                        save_path = '/home/g2015samtaylor/illuminate',\n",
    "                        view_path = '/home/g2015samtaylor/views',\n",
    "                        years_data = '24-25',\n",
    "                        start_date = '2024-07-01')\n",
    "\n",
    "# end_date_override='2024-07-01' #should default to todays date\n",
    "\n",
    "\n",
    "\n",
    "#Create spark session in main script\n",
    "#Merge branch with main for feauture enhancement practice. \n",
    "\n",
    "#Add to requirements.txt\n",
    "#Re-initaite docker with new tag of spark, in case need to roll back\n",
    "#Update changes in docker file\n",
    "#Make sure changes flow through to airflow\n",
    "#Run locally for string matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file = '/home/g2015samtaylor/illuminate/test_file.csv'\n",
    "df = pd.DataFrame()\n",
    "df.to_csv(new_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from modules.frame_transformations import *\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "fixes = pd.read_csv('/home/g2015samtaylor/airflow/git_directory/Illuminate/illuminate_historical_column_fixes_2324.csv')\n",
    "v = pd.read_csv('/home/g2015samtaylor/views/illuminate_assessment_results_historical.csv') \n",
    "\n",
    "#Birng back jennys produced excel frame with the master assessments frame\n",
    "access_token, expires_in = get_access_token()\n",
    "temp, assessments_df = merge_excel_with_assessments_master_on_title(access_token)\n",
    "\n",
    "#Right only are titles that are strictly in the produced excel file\n",
    "#Present means the titles are present in teh excel file and the assessments endpoint\n",
    "missing = temp.loc[temp['_merge'] == 'right_only']\n",
    "present = temp.loc[temp['_merge'] == 'both']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(present[['title', 'current grade', 'updated grade', 'curriculum', 'updated curriculum']].tail(4)) # 130 total titles present\n",
    "\n",
    "print('Here is the view below with the title present')\n",
    "v.loc[v['title'].str.contains('Kinder - IM Unit 5 Checkpoint A',case=False)][['title', 'grade', 'curriculum']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(missing[['title']])\n",
    "\n",
    "print('Teseting if partial string is present')\n",
    "assessments_df.loc[assessments_df['title'].str.contains('1st Grade IM Unit', case=False)]['title']\n",
    "\n",
    "#Looks like 132 of the titles are completely missing from Illuminate when I pull from their assessments endpoint. \n",
    "#These are titles that originagte from the results-20250106-085115 excel file. \n",
    "\n",
    "#Without any matching title, or matching assessment_id I am unsure of what to do with these moving forward. \n",
    "\n",
    "# A couple of questions is where did you get these title names from? Can you obtain the assessments ids for these if so?\n",
    "\n",
    "#The strings matching or assessment_id matching is possible when the titles are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assessments_df.loc[assessments_df['title'].str.contains('Kinder - IM Unit 7', case=False)]\n",
    "#Missing Kinder - IM Unit 6 Checkpoint B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assessments_df.loc[assessments_df['title'].str.contains('Kinder - IM Unit 8', case=False)]\n",
    "#Missing edn of unit assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-testing and double checking get_all_assessments_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://demo.illuminateed.com/live/rest_server.php/Api/Assessments/\n",
    "\n",
    "# Set the initial page and an empty DataFrame to store all results\n",
    "page = 1\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "#To ensure all pages are looped through properly\n",
    "# while True:\n",
    "\n",
    "#Base URL and headers for API requests\n",
    "url_ext = f'Assessments/?page={page}&limit=1000'\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "\n",
    "logging.info(f'Fetching data from {base_url_illuminate + url_ext}')\n",
    "\n",
    "# try:\n",
    "    # Make the API request with the current page number\n",
    "response = requests.get(base_url_illuminate + url_ext.format(url_ext), headers=headers)\n",
    "\n",
    "#From raw request here are the outputs\n",
    "# 'page': 1,\n",
    "#  'num_pages': 2,\n",
    "#  'num_results': 1126,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token, expires_in = get_access_token()\n",
    "assessments_df, assessment_id_list = get_all_assessments_metadata(access_token)\n",
    "\n",
    "#All possible assessments from this endpoint are grabbed\n",
    "assessments_df['assessment_id'].nunique() == 1126"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efforts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Based on complete string matching of the title column between results-20250106-085115 excel file & all of the assessments found at the  in the views db.\n",
    "\n",
    "#When merging with the illuminate_assessments_results there are 160 titles that are present in the illuminate_assessments_results table & 132 that are not are not found.\n",
    "\n",
    "#Becuase these are strictly in the produced excel file, hence there is string matching that can occur on the title.\n",
    "\n",
    "------------------------------------------\n",
    "# Example - because titles with IM has multiple grades I can not apply a singule updated grade to all based on IM being in the title\n",
    "# However it will work for updated curriculum. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
